[Diary] 2021-08-02

I have been looking at so many different stuffs recently, I feel if I don’t make a summary or something my brain gonna explode.

I was trying to learn some computatbility theory, read stuffs about hilbert’s program, godel’s incompletenss theory, and turing machine. (Stanford philosophy encylopydia is a very good source!). The most important take away is probably church turning thesis can’t be proved, so we really shouldn’t claim that turing machine can solve all computable problems. The common claim is better concluded by maximality theorem, but people who dare to use these terms perfer to shock ppl rather than maintain the accuracy of their statements, which just leads things to mysterianism. When turing was trying to design turing machine, the design was originated from the capabilities of human computers. Tape to memory, state to knowledge, symbols to, ... symbols! There are finite amount of symbols because a human knonws finite amount of characters! There is no formal definition for things that a human can computte, thus no formal definition for effectively calculatable functions. You can’t prove something informal.

Then  I was trying to implement a simple busy beaver. Luckily it’s simple enough that I can finish it in one sit. You can’t tell if a program halts, so you can’t really know what’s the longest busy beaver unless you try each one out.

Then I read about oracle machine somewhere, and saw ppl mimic an oracle with ANN, then I was trying to crank up an ANN myself in haskell. Well, not hard right, just bunch of lineare combinations, after get the prediction vetor, run gradient descend to get a delta vector, then back propagates to turn the model. The processing of training is like oiling an rusty machine, slowly make it runs. The model? There is no model, it’s just buch of polynomials and constants.

After reading of machine learning arcticles, all the sudden I found I don’t know how to do type level programming in Haskell, at least no as comfortable as in c++. Then I spend a week just to learn relevant knowldeges. I read about GADT inferences, typeclass tricks, type family ticks, bunch of quirks that only haskell has etc. It starts to feel more and more like C++, in the sense that features get hacked together to provide type level programming. If all these things exists in dependent type lanaguages already, why don’t just use the same terminology and say what they are direclty?

Then somehow I decided to learn some purely functional data structures. I don’t why, random, right. Like how do you analyse the complexity of these things? When reading the first chapter of the book by okasaki (It seems everybody remeber his name I sus it’s beacuse it’s very recognizable), it mentions amortization a lot. I guess I don’t know much about that. Of course you go ahead and learn some. Banker’s method, potential method, and applying the method with some examples, like 23 trees or somthing. The gist is if you claim the complexity is say O(N), then all operations smaller then O(N) contributes to these claim, and occasional expensive operation needs to be compensated by enough simple operations to allow you to claim the the amortized bound is O(N).

Oh btw23 tree, I’ve never implemneted one, the concept is kind similar to B tree, you split and propagates up wards. Very qute data structure, let’s make one... Then I found 23 tree is actually related to finger tree. Hmm, finger tree, I know it has amortized O(1) for almost anything, and it has the funny shape of pulled up tree. Interesting, should I read the paper?

Oh, I forgot to mention before all these I was trying to implemnet peter J landin’s SECD. I saw an implmention with clojure, the logic is implement into a dsl, and the actual machine is just written as if writing the spec for the abstract machine! Pretty cool technique, I want to do a similar one in common lisp. Wait common lisp? What a terrible language. Maybe I should try racket?

And somehow in between I tried lean, it feels like haskell done right. Tried some proofs, I feel it’s roughly the same verbosity as Agda. But well, I just tried some examples so it’s really inconclusive. Right, before I do this, I was learning martin lof type system. Apparently all dependent type system are more or less variation of martin lof type system, and it’ the type system in the curry howard isomorphism.

And before all of this, I was doing two things.. 1. practicing some leetcode because I might need to find a job. It’s not very hard but I feel it’s not enlightening; 2. learning database. Right, I read half of the “database internal”, famous book, just to get educated in this matter.

All of these happend after he left. July 14? It’s only been 2 weeks? Gee!

Let me think what did I do before July 14.. I learnt some rust, tried to write template lambda calculus, terms out it was not that fun either. Oh yeah I was spending lot of time to learn some boomer programming languages. Algol and stuffs. Pretty interesting that earilest four high level programming langauges came out in consecutive 4 years. I think it was fortran first in 1957? Then cobal 1958, lisp 1959, and algol 1960. Maybe cobal and lisp was reversed. But anyway. It’s wierd to look at these old research, sometimes they are researching something super advance, but sometimes they don’t even have the basic stuffs sorted. Lisp came with dynamic scope still triped me off. How can anybody wirte program with dynamically scoped languages? Then you see ppl
