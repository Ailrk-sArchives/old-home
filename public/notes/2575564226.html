<h1>Clustering</h1>
<h2>Unsupervised learning</h2>
<p>Algorithms that looking for patterns of the unlabeled data without human intervention.</p>
<h2>Metrics</h2>
<ul>
<li>Euclidean distance: || a - b ||₂ = sqrt(∑(aᵢ-bᵢ)²)</li>
<li>Squared Euclidean distance: ||a - b||₂² = ∑(aᵢ-bᵢ)²</li>
<li>Manhattan distance: ||a - b||₁ = ∑|aᵢ-bᵢ|</li>
<li>Maximum distance: ||a - b||∞ = max|aᵢ-bᵢ|</li>
</ul>
<h2>SSE: Sum of squared error.</h2>
<p>This is a very common metric for assessing distance between points.</p>
<p>SSE: the sum of squared differences between each observatino and it's group's mean.  It is used as a measure of variation within a cluster. If all casese in a cluster are identical, in another word, points overlap at one poing, then sse will be zero.</p>
<pre><code>      n
SSE = ∑ (xᵢ - avg x)²
     i=1
</code></pre>
<h2>Clustering</h2>
<h2>KMeans</h2>
<h5>PS</h5>
<p>All this machine learning methods have their theorotical goals and different implementation. The acutal implementation can be very different from their conceptual goal, os it is helpful to describe them in two different sections.</p>
<h5>Conceptually</h5>
<p>Kmean aims to choose entroids that minimize the intertia (within cluster sum of squares).</p>
<p>let C be set of K disjointed clusters</p>
<pre><code>    xᵢ be the input data. {x₁, x₂, ... xᵢ}
    μⱼ be the mean of samples in cluster
    n is the number of input

</code></pre>
<p>for every j ∈ K, we want to find μⱼ that satisfy this constraint:</p>
<pre><code>                   n
    J(μ₁ .. μₖ) =  ∑ min(||xᵢ - μⱼ||²), μⱼ ∈ C
                  i=0
</code></pre>
<p>This is the cost function, which is kinda different from pure function. With a normal pure func you take an argument and substitue it into the function body, the result of the evaluation is what you want to get.</p>
<p>But for the cost function, you can do similar thing: get a set of specific arguments and evaluate the function. But now the result of evaluation is not what your goal anymore. You want to find a specific set of arguments that can minimize the result of evaluation.</p>
<p>So really, the argument is the unknown, and we're trying to find the answer backward.</p>
<h5>Implementation</h5>
<ul>
<li>Mean of points means take average of the result of vector addition.</li>
<li>It's a iterative algorithm with two steps.</li>
<li>Some times the result will not convergent, and we need to set a tolerant threshold.</li>
</ul>
<pre><code>init K clusters centroids {μ₁, μ₂, ... , μₖ} randomly.
while {
}

// assign cluster centroids with cloest sample data points.
def assign {
    for i = 1 to m
       cᵢ := index of cluster centroids closet to xᵢ
}

// relocate cluster centroids with mean all assigned data points.
def step {
    for k=1 to K
       μₖ := mean of points assigned to cluster k
}
</code></pre>
<h5>Optmization</h5>
<p>Random initialization is possible to cause the resulting centroids end up at local optima and yield different solutiont from what we expect.</p>
<p>To solve this problem we try random initialization mutiple times, and pick the one with the lostest cost. (Yeah, brute force)</p>
<p>It's said this is not a big performance problem. Stuck at local optima typically happen when data set is small, since when data set is large the chance of data being stucked is much smaller. One can also dynamically adjust the amount of iterations based on sample size.</p>
<h5>Choose number of cluster</h5>
<p>Most common thing to do is to choose the number of clusters by hand...</p>
<p>There is even a heuristic method called elbow method to help you choose ncluster.</p>
<p>On another hand, the number of clusters can actually be chosen by your business logic! For example, if you want to partition a data set into 3 different groups so you can produce different versions of product, you know nclusters already!</p>
<h2>Hierarchy Agglomerative Clustering.</h2>
<ul>
<li>Agglomerative: bottom up</li>
<li>Divisive: top down</li>
</ul>
<p>The grouping idea is like fast labeling algorithm, smaller cluster eventually merge to a big cluster.</p>
<p>complexity O(n³), very slow.</p>
<p>The clustering eventually produce ea dendrogram. It's actually a trie.</p>
<h6>Procedure</h6>
<p>Calculate the distance between each points and sort it. group the cloest one and repeat.</p>
<h2>K-Medoids</h2>
<ul>
<li>Kmean is sensitive on outliner</li>
<li>Kmedoids choose data points as centers, instead of like kmean use arbitrary points.</li>
<li>O(k(n-k)²) -&gt; O(n²)</li>
</ul>
<h5>Procedure</h5>
<pre><code>PAM algorithm:

1. Init k medoids.
2. Assign each data point to the closest medoid.
3. while sse decrease, for each non medoids o, swap each amedoids m and o until the sse is the current best. Then perform the best swap.
</code></pre>
<pre><code>Voronoi iteration
1. Selecet initial medoids randomly
2. Iterate while the cost decrease:
    1. In each cluster, make the point the minimizes the sun of distance the medoid
    2. Reassing.
</code></pre>
