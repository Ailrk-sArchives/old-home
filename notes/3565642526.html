<h2>LLVM</h2>
<p>LLVM has bunch of subprojects, which make it hard to navigate for newbies like me. Normally when people address LLVM <code>ir</code> they are talking about the <a href="https://github.com/llvm/llvm-project/tree/master/llvm/lib"><code>llvm-core</code></a> library. It's the library that provides code generations and <code>target and source</code> independent optimizations. Many of other subprojects are around providing support for <code>clang</code>, which is not really important for writing your own compiler.</p>
<h3>Classical compiler design</h3>
<p><code>Frontend -&gt; Optmizer -&gt; Backend</code>, Compiler archtecture has been like this for eon. Frontend parses the source code and transfrom it into AST. Errors like naming conflict, type error, and syntax error can be caught at this stage (not necessary to be here. For some compiler typechecking is done on the typed ir). Some frontend convert ast into ir, some doesn't, it all depends on how the backend is structured. The optimizater takes the ir or ast as the input, and perform bunch of code transformations to improve the performance. Optimizations can be performed at this stage is restricted to those that are platform independent. That is, it only takes care about proving the logic of the program, but it cannot take the advantage of some specific benefits provided by the target architecture. (It's kinda like writing babel transform, you take the ast, find the pattern and shove another version of the code into it. Or maybe babel plugin is a way to write customized optimizer..)</p>
<h3>LLVM overview</h3>
<h4>Benefits</h4>
<pre><code>C ---->--+                                + x86 back end   -> x86
         |                                |
Haskell -+-> llvm-ir -> llvm optimizer -> +  arm back end  -> arm
         |                                |
Rust ->--+                                + riscv bank end -> riscv
</code></pre>
<p>LLVM provides a common IR format that different compiler frontend can compile to , and it will handle the rest of the compilation. The ir is easy to target, and llvm provides thorough solution for llvm-ir optimization and code-generation, the binary get generate can be very efficient. It's like a lsp situation, you have the llvm-ir as the middle layer to bridge between frontend and backend, so you can take the advantage of the established backend that supports llvm-ir.</p>
<p>So now if you want to write a compiler targeting llvm-ir,you are really just writing the front-end,</p>
<h4>How people use compiler archtectures of other languages?</h4>
<p>The goal of llvm is to have a reusable compiler archtecture to deal with common tasks compiler will face. The incentive arises from the fact that most of compilers have their specific implementations for the entire pipeline, and most of them are deeply coupled between each components, which makes other compiler that wants to reuse them more difficult.</p>
<p>One way to reuse compiler archtecture is to compile to C. The compiler can just treat c as an IR, and use gcc to generate target code. This approach is not as terrible as it seems, but it does make debugging harder and impose some limitations that you will only have with C (no tail recursion). GHC can target to C, but it has bad performance and it is generally used to port GHC to other platforms.</p>
<p>Java bytecode is a very good target if you want to utilize all the efforts people put into <code>JVM</code> runtime, but <code>JVM</code> is highly optimized and deeply tight with java's object model, which can leads to suboptimal code for languages don't follow the same model.</p>
<p>For gcc, it has standalone IR called <code>GIMPLE</code>, which can potentially be a good target, but because gcc is designed to be monolithic, GIMPLE can not be used as a standalone library, if you want to target gcc's internal representation you must pull the inards out.</p>
<p>There are a lot of ways to hack around, but llvm helps provide a standard solution so you don't need to do hacky stuffs.</p>
<h2>LLVM IR</h2>
<h4>LLVM IR form</h4>
<p>An example of LLVM IR. LLVM IR is statically typed, well defined languaged. Compiler front end compiles source code into LLVM IR, and all the thing the optimizer needs to do is to optimze and code gen the IR.</p>
<pre><code class="language-llvm"><span class="hljs-keyword">define</span> <span class="hljs-keyword">i32</span> <span class="hljs-title">@add1</span>(<span class="hljs-keyword">i32</span> <span class="hljs-symbol">%a</span>, <span class="hljs-keyword">i32</span> <span class="hljs-symbol">%b</span>) {
entry:
    &amp;tmp<span class="hljs-number">1</span> = <span class="hljs-keyword">add</span> <span class="hljs-keyword">i32</span> <span class="hljs-symbol">%a</span> <span class="hljs-symbol">%b</span>
    <span class="hljs-keyword">ret</span> <span class="hljs-keyword">i32</span> <span class="hljs-symbol">%tmp1</span>
}
</code></pre>
<p>LLVM IR has RISC style three address form with infinite registers plus some other high level constructs. You can do some low level operatoins like move a value from one register to another, but you don't need to follow the calling convention everytime calls a funciton.</p>
<p>LLVM IR is defined in three isomorphic formates, namely <code>.ll</code>, <code>.bc</code> are two textual formates for source code and byte representation respectively. You can transform from on format to another with <code>llvm-as</code> and <code>llvm-dis</code>.<br>
e</p>
<h4>IR</h4>
<p>IR design needs to be easy to be generated by the front end while expressive enough to provide enough information for the optimizer to perform optimization. <code>Core</code> language in GHC is another example of statically typed IR, because haskell is a lambda calculus based language, <code>Core</code> is largely how haskell looks like when it is desugared into typed lambda calculus. Same as llvm ir, a lot of optimizations are carried out on the <code>Core</code> representation before it move to the code generation stage.</p>
<h2>Optimization</h2>
<h4>Optimization is a bag of code transforms.</h4>
<p>Optimization is like a umberella term for anything that improves the runtime performance, and there are tons of code optimization strategies you can choose. Most optimizations are in the form of code transformation. You analyse the IR, find patterns that match with one of your optimization, and then you run the optimization to transform the source code to something better.</p>
<p>For example, some optimizations like <code>turn x-0 to x</code> and <code>turn x-x to 0</code> can be easily implemented: simply find expressions in the ir that have form <code>x-0</code> and turn it into x. Note it can be modeled easily with pattern matching, but because llvm is implemented with c++, it needs some specific apis to achieve the similar effect.</p>
<h2>LLVM design decisions.</h2>
<h4>LLVM has first class textual form.</h4>
<p>It's worth noting that lots of IR doesn't have textual representation or have clumsy support for that (namely gcc). So really the only place you will use your IR is in the compiler. With this limitation you cannot transform the code easily, you need soem specific apis to generate IR. A well defined textual format means easier to rewire the whole compiler pipeline and adopt changes. You can generate llvm ir with some randome program on linux, pipe it into llvm optimizer, and you suddenly get an optimized target code.</p>
<h4>LLVM been designed as collection of libraries.</h4>
<p>As opposed to other monolithc compilers and VMs like gcc and JVM, LLVM is designed as a set of libraries that each specifc functionalities can be requested individually.</p>
<p>For instance, llvm optimizer is organized as a pipeline of different optimization passes, each pass might run on the ir and change some part of it. There are passes like inliner, expresino reassociation, loop invariant code motion etc, different optimization level will tigger different optimization passes. (on -O3 clang will run 67 passes).</p>
<p>Each llvm pass are compiled separatly into different <code>.o</code> files, and they are designed intentionally to be decoupled from each other. Most passes can run on their own. If one can't, it needs to explicitly specify it's dependency. Upon exection this isolated passes will be managed my <code>PassManager</code>, which can resolve the explicit dependency.</p>
<p>With the library based design of optmizer, when writing you own compiler you can pick not only which pass to use but the order of execution. This gives you much better granularity of control over the compiling process. For instance, one use case of it is to allow turn off some optimization passes to speed up compilation, and it is especially useful for compilers that has high requirement on latency.</p>
<p>Another benefit of this library approach is to reduce the size of the final compiler. Since you are linking only parts that you're interested, you don't need to include the entire llvm project like how people compile to gcc.</p>
<h4>LLVM retargetable code generator</h4>
<p>Although code generator are responsible for generating the most optimized code for each specific target, there are lots of common logics between different code generators for different targets. Each target needs to assign values to registers, and desipte different targets might have different register files, the algorithm for checking those register files is the same.</p>
<p>Like the optimizer, LLVM has multiple passes for code generation. For instance, instruction selection, register allocation, scheduling, code layout optimization, assembly smission and so on. Some general purpose passes can be reused between different generators, and some more specific passes are made for specific targets. For instance, x86 has it's special pass for handling floating point stack, and the ARM back end has a custom pass to handle <a href="https://en.wikipedia.org/wiki/Literal_pool">constant pool islands</a>.</p>
<h6>LLVM target descrption flies</h6>
<p>The approach of providing generic passes for common operations implies for each specific target the generic pass will need some extra information. For example, a generic register allocation algorithm will need to know the register file of each target. LLVM has a small declarative DSL <code>.td</code>, which describes target specific information for generic modules.</p>
<h4>Benefits gained from Modular design</h4>
<h5>Link time optimization</h5>
<p>Traditionally compiler cannot optimize across the file boundary. The optimization will only happen within individual compilation units, and at the last stage all compiled object files will be linked to form one binary. Link time optimization delay the optimization time after compilation units get linked, so that the compiler can see a bigger picture about the entire program. Doing so allows compiler to achieve more aggressive dead code elimination, inline, etc. (this is enabled by <code>-O4</code> option in clang).</p>
<p>How llvm do LTO is to generate ir and serialize them into <code>.o</code> while compiling for individual compilation unit. During the link time, if the compiler find out the <code>.o</code> file it is linking is actually LLVM IR, it will optimize it again, this time with more information about proximite code. Finally the linked ir will be sent to the code generator.</p>
<h5>Unit testing the optimizer</h5>
<p>It's easier to test the robustness of llvm source code because of it's modular design. Since stuffs can be pulled out easily, it's straight forward to write unit test for each individual functionalities.</p>
<h5>Conclusion</h5>
<p>The core feature for LLVM is it's generic IR and it's modular design. The archtecture provides a robust framework that not only easier to maintain, but also provides great flexibilities for code reuse: expose more functionalities, and let users decide what piece to use.</p>
